At the start of the Aggregator scenario, we configured Vector to send data off to nowhere using the
`blackhole` sink. This is clearly not a production-ready configuration! So let's configure some real
sinks for our data. We won't actually send that data anywhere, but we will get a feel for how to use
multiple sinks.

## AWS S3 archival

One common approach for storing observability data is to store everything in an inexpensive **system
of record** that you can potentially pull data from later and only *some* higher-value data in a
SaaS service.

<pre class="file" data-filename="aggregator/vector/aggregator/vector.toml" data-target="insert" data-marker="#insert-s3-sink">[sinks.s3_archive]
type = "aws_s3"
inputs = ["route_logs.all"]
bucket = "observability-data"
key_prefix = "date=%F/"
region = "us-east-1"</pre>

## Datadog sink

Up above, we configured our Aggregator to send all our logs (via the `route_logs.all` route) in AWS
S3. Now let's send our high-value logs (via the `route_logs.high_value` route) to Datadog:

<pre class="file" data-filename="aggregator/vector/aggregator/vector.toml" data-target="insert" data-marker="#insert-datadog-sink">[sinks.logs_to_datadog]
type = "datadog_logs"
inputs = ["route_logs.high_value"]
default_api_key = "${DD_API_KEY}"
region = "us"
</pre>

## Metrics

And finally, we have our metrics. Let's say that we don't to use Datadog for metrics but rather a
service that accepts Prometheus [remote write][remote].


<pre class="file" data-filename="aggregator/vector/aggregator/vector.toml" data-target="insert" data-marker="#insert-prometheus-sink">[sinks.metrics_out]
type = "prometheus_remote_write"
inputs = ["tag_metrics"]
endpoint = "https://api.prometheus-saas-service.io"</pre>

In our Aggregator, we've actually *replaced* Prometheus.

[remote]: https://prometheus.io/docs/operating/integrations/#remote-endpoints-and-storage