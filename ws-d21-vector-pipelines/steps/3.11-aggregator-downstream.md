At this point in our scenario, our Vector Aggregator is sending data to S3, Datadog, and a service
compatible with Prometheus remote write, like this:

```
Agents  --->  Aggregator  --->  AWS S3
                          \-->  Datadog
                          \-->  Prometheus remote write
```

Another possibility to bear in mind, however, is that we could also send data to **another
Vector Aggregator**, like this:

```
Agents  --->  Aggregator  --> Aggregator --> Sink(s)
```

We might do this if we wanted to, for example, run one Aggregator in each availability zone
in a datacenter and then send that data to a centralized Aggregator for the entire datacenter.

Here's an example sink that would send all logs to another Vector Aggregator:

<pre class="file" data-filename="aggregator/vector/aggregator/vector.toml" data-target="insert" data-marker="#insert-aggregator-sink">
[sinks.downstream_vector_aggregator]
type = "vector"
inputs = ["route_logs.all_logs"]
address = "vector_downstream:6000"
version = "2"</pre>

That downstream Aggregator could then accept data from that Aggregator just like we did in our
scenario:

```toml
[sources.upstream_vector_aggregator]
type = "vector"
address = "0.0.0.0:9000"
version = "2"
```

In fact, Aggregators can be chained together indefinitely:

```
Agents  --->  Aggregator  --> ...Aggregator --> Sink(s)
```

This means that we could have one Aggregator per availability zone sending data to one central
Aggregator per datacenter, which then sends data to one central Aggregator per continent. The sky's
the limit. You can build out your Vector pipelines however your use cases demand and distribute
observability data processing to fit just about any network topology.
