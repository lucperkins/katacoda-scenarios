At the start of the Aggregator scenario, we configured Vector to send data off to nowhere using the
`blackhole` sink. This is clearly not a production-ready configuration! So let's configure some real
sinks for our data. We won't actually send that data anywhere, but we will get a feel for how to use
multiple sinks.

## AWS S3 archival

One common approach for storing observability data is to store everything in an inexpensive **system
of record** that you can potentially pull data from later and only *some* higher-value data in a
SaaS service.

<pre class="file" data-filename="aggregator/vector/aggregator/vector.toml" data-target="insert" data-marker="#insert-s3-sink">[sinks.s3_archive]
type = "aws_s3"
inputs = ["route_logs.all_logs"]
bucket = "observability-data"
key_prefix = "date=%F/"
region = "us-east-1"
encoding = { codec = "ndjson" }</pre>

## Datadog sink

Up above, we configured our Aggregator to send all our logs (via the `route_logs.all_logs` route) in AWS
S3. Now let's send our high-value logs (via the `route_logs.high_value_logs` route) to Datadog:

<pre class="file" data-filename="aggregator/vector/aggregator/vector.toml" data-target="insert" data-marker="#insert-datadog-sink">[sinks.logs_to_datadog]
type = "datadog_logs"
inputs = ["route_logs.high_value_logs"]
default_api_key = "${DD_API_KEY}"
region = "us"</pre>