At the start of the Aggregator scenario, we configured Vector to send data off to nowhere using the
`blackhole` sink. This is clearly not a production-ready configuration! So let's configure some real
sinks for our data.

## AWS S3 archival

One common approach for storing observability data is to store everything in a **system of record**

<pre class="file" data-filename="aggregator/vector/aggregator/vector.toml" data-target="insert" data-marker="#insert-s3-sink">[sinks.s3_archive]
type = "aws_s3"
inputs = ["*"]
bucket = "observability-data"
key_prefix = "date=%F/"</pre>

## Datadog sink

Up above, we configured our Aggregator to store all and sundry in AWS S3.

<pre class="file" data-filename="aggregator/vector/aggregator/vector.toml" data-target="insert" data-marker="#insert-datadog-sink">[transforms.filter_out_unimportant]
type = "filter"
inputs = [""]</pre>

<pre class="file" data-filename="aggregator/vector/aggregator/vector.toml" data-target="insert" data-marker="#insert-prometheus-sink">[sinks.prometheus_remote]
type = "aws_s3"
inputs = ["*"]
bucket = "observability-data"
key_prefix = "date=%F/"</pre>